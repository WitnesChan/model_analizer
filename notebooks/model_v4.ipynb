{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "ENV_COLAB = 'google.colab' in sys.modules\n",
    "FILE_NAME = 'DATASET_THESIS_2022.csv'\n",
    "\n",
    "if ENV_COLAB:\n",
    "\n",
    "  DIR = '/content/drive/My Drive/Thesis Project/'\n",
    "  from google.colab import output, drive\n",
    "  !pip install feature_engine hyperopt shap\n",
    "\n",
    "  output.enable_custom_widget_manager()\n",
    "  drive.mount('/content/drive/')\n",
    "\n",
    "else:\n",
    "\n",
    "  DIR = './'\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score   \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale, quantile_transform\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, \n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    )\n",
    "\n",
    "# for feature engineering\n",
    "from feature_engine import imputation as mdi\n",
    "from feature_engine import encoding as ce\n",
    "from feature_engine import discretisation as disc\n",
    "from feature_engine import transformation as t\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([chunk for chunk in tqdm(pd.read_csv(DIR + FILE_NAME, chunksize=1000), desc='Loading data')])\n",
    "y = df_data.TARGET\n",
    "X = df_data.drop(columns=['TARGET'])\n",
    "feat_names = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state= 42)\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#                 ('imputer', mdi.MeanMedianImputer(imputation_method='median')),\n",
    "#             ])\n",
    "# X_train = pipe.fit_transform(X_train)\n",
    "# X_test =  pipe.transform(X_test)\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "d_test = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "                ('imputer', mdi.MeanMedianImputer(imputation_method='median')),\n",
    "            ])\n",
    "X_train = pipe.fit_transform(X_train)\n",
    "X_test =  pipe.transform(X_test)\n",
    "\n",
    "mdl_rf = RandomForestClassifier(\n",
    "    random_state= 43,  n_estimators= 300, max_samples = 0.8280, criterion= 'entropy', max_depth= 8, bootstrap= True, class_weight= 'balanced')\n",
    "\n",
    "mdl_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mdl_xgb = pickle.load(open('xgb_mdl_v1.pkl', 'rb'))\n",
    "params = {'base_score': 0.5,\n",
    " 'booster': 'gbtree',\n",
    " 'colsample_bylevel': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'gamma': 0.49999999999999994,\n",
    " 'learning_rate': 0.08203175775371215,\n",
    " 'max_delta_step': 0,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 1,\n",
    " 'missing': None,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': 1,\n",
    " 'nthread': None,\n",
    " 'objective': 'binary:logistic',\n",
    " 'random_state': 43,\n",
    " 'reg_alpha': 0,\n",
    " 'reg_lambda': 1,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': None,\n",
    " 'silent': True,\n",
    " 'missing': 1,\n",
    " 'subsample': 1}\n",
    " \n",
    "mdl_xgb = xgb.XGBClassifier(**params)\n",
    "\n",
    "mdl_xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_xgb.set_params(**{'missing': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = mdl_rf.predict_proba(X_test)\n",
    "print('AUC: ',roc_auc_score(y_test, pred_test[:,1]))\n",
    "print('accuracy_score: ', accuracy_score(y_test,  mdl_rf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "mdl_cat = CatBoostClassifier(iterations=10)\n",
    "mdl_cat.load_model('catboost_model.bin')\n",
    "\n",
    "mdl_cat.fit(X_train,  y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_feat_imp_per = pd.Series(res.importances_mean, index = feat_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "\n",
    "ser_feat_imp_per.sort_values(ascending= False).head(10).plot.barh(yerr = res.importances_std[:50])\n",
    "ax.set_title('Feature importances using permutation on full model')\n",
    "ax.set_ylabel('Mean accuracy in impurity')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl_xgb = pickle.load(open('xgb_mdl_v1.pkl', 'rb'))\n",
    "params = {'base_score': 0.5,\n",
    " 'booster': 'gbtree',\n",
    " 'colsample_bylevel': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'gamma': 0.49999999999999994,\n",
    " 'learning_rate': 0.08203175775371215,\n",
    " 'max_delta_step': 0,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 1,\n",
    " 'missing': None,\n",
    " 'n_estimators': 50,\n",
    " 'n_jobs': 1,\n",
    " 'nthread': None,\n",
    " 'objective': 'binary:logistic',\n",
    " 'random_state': 43,\n",
    " 'reg_alpha': 0,\n",
    " 'reg_lambda': 1,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': None,\n",
    " 'silent': True,\n",
    " 'missing': 1,\n",
    " 'subsample': 1}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_mdl_scores = []\n",
    "\n",
    "thresholds = sorted(mdl_xgb.feature_importances_)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(mdl_xgb, threshold=thresh, prefit=True)\n",
    "\n",
    "    # train model\n",
    "    selection_model = xgb.XGBClassifier(n_jobs = 3)\n",
    "    selection_model.fit(selection.transform(X_train), y_train)\n",
    "\n",
    "    # eval model\n",
    "    prob = selection_model.predict_prob(selection.transform(X_test))[:,1]\n",
    "\n",
    "    auc = roc_auc_score(y_test, prob)\n",
    "    selected_mdl_scores.append([thresh, select_X_train.shape[1], auc])\n",
    "\n",
    "    print(\"Thresh=%.3f, n=%d, AUC: %.2f%%\" % (thresh, select_X_train.shape[1], auc*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logit_params = {'C': 2.655690779733333, 'penalty': 'l1', 'solver': 'saga', 'class_weight': 'balanced'}\n",
    "\n",
    "logit_pipe = Pipeline([\n",
    "        ('imputer', mdi.MeanMedianImputer(imputation_method='median')),\n",
    "        ('scaler', StandardScaler(with_std= True)),\n",
    "        ('logit', LogisticRegression(random_state= 43, **best_logit_params))\n",
    "    ])\n",
    "\n",
    "logit_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preds = logit_pipe.predict_proba(X_train)[:,1]\n",
    "X_test_preds = logit_pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\n",
    "print('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(abs(logit_pipe[2].coef_)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "median_selector = SelectFromModel(estimator= logit_pipe[2], threshold= 'median' , prefit= True)\n",
    "\n",
    "median_selector.transform(X_train).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## catboost feature importance \n",
    "\n",
    "df_feat_cat = pd.read_excel('feature_importance_catboost.xlsx')\n",
    "feat_cat_set = set(df_feat_cat[df_feat_cat.PredictionValuesChange > 0]['Feature Id'])\n",
    "df_feat_cat = df_feat_cat.set_index('Feature Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    " 'class_weight': 'balanced',  \n",
    " 'boosting_type': 'gbdt',\n",
    " 'colsample_by_tree': 0.9584435916048172,\n",
    " 'learning_rat': 0.3,\n",
    " 'max_depth': 15,\n",
    " 'n_estimators': 200,\n",
    " 'num_leaves': 30,\n",
    " 'reg_lambda': 0.32826016348987364,\n",
    " 'max_bin' : 255\n",
    " }\n",
    "\n",
    "mdl_lgb = lgb.LGBMClassifier(**params)\n",
    "\n",
    "mdl_lgb.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], eval_metric='auc', verbose = 10)\n",
    "\n",
    "X_test_preds = mdl_lgb.predict_proba(X_test)\n",
    "print('AUC: ',roc_auc_score(y_test, X_test_preds[:,1]))\n",
    "print('accuracy_score: ', accuracy_score(y_test,  mdl_lgb.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_dt = DecisionTreeClassifier(random_state= 43, max_depth= 5, class_weight='balanced')\n",
    "\n",
    "mdl_dt.fit(X_train, y_train)\n",
    "\n",
    "X_test_preds = mdl_dt.predict_proba(X_test)\n",
    "print('AUC: ',roc_auc_score(y_test, X_test_preds[:,1]))\n",
    "print('accuracy_score: ', accuracy_score(y_test,  mdl_dt.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peformance Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl_perf = pd.DataFrame(\n",
    "    data = {\n",
    "    'Logistic Ridge': [\n",
    "        roc_auc_score(y_train, logit_pipe.predict_proba(X_train)[:,1]),\n",
    "        roc_auc_score(y_test, logit_pipe.predict_proba(X_test)[:,1])\n",
    "        ],\n",
    "    'Decision Tree': [\n",
    "        roc_auc_score(y_train, mdl_dt.predict_proba(X_train)[:,1]),\n",
    "        roc_auc_score(y_test, mdl_dt.predict_proba(X_test)[:,1])\n",
    "        ],\n",
    "    'Random Forest':[ \n",
    "        roc_auc_score(y_train, mdl_rf.predict_proba(X_train)[:,1]),\n",
    "        roc_auc_score(y_test, mdl_rf.predict_proba(X_test)[:,1])\n",
    "        ],\n",
    "    'XGBoost': [\n",
    "        roc_auc_score(y_train, mdl_xgb.predict_proba(X_train)[:,1]),\n",
    "        roc_auc_score(y_test, mdl_xgb.predict_proba(X_test)[:,1])\n",
    "        ],\n",
    "    'Lightgbm': [\n",
    "        roc_auc_score(y_train, mdl_lgb.predict_proba(X_train)[:,1]),\n",
    "        roc_auc_score(y_test, mdl_lgb.predict_proba(X_test)[:,1])\n",
    "        ],\n",
    "    'Catboost': [\n",
    "        roc_auc_score(y_train, mdl_cat.predict_proba(X_train)[:,1]),\n",
    "        roc_auc_score(y_test, mdl_cat.predict_proba(X_test)[:,1])\n",
    "        ]\n",
    "    },\n",
    "    index= ['train_auc', 'test_auc']\n",
    ").T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(mdl_lgb, max_num_features= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_feat_xgb = pd.DataFrame(\n",
    "    {\n",
    "        'feat_name' : feat_names,\n",
    "        'xgboost': mdl_xgb.feature_importances_\n",
    "    }\n",
    ").set_index('feat_name')\n",
    "\n",
    "df_feat_logit = pd.DataFrame(\n",
    "    {\n",
    "        'feat_name': feat_names,\n",
    "        'logistic': abs(logit_pipe[2].coef_)[0]\n",
    "    }\n",
    ").set_index('feat_name')\n",
    "\n",
    "\n",
    "df_feat_lgb = pd.DataFrame(\n",
    "    {\n",
    "        'feat_name': feat_names,\n",
    "        'lightgbm': mdl_lgb.feature_importances_\n",
    "    }\n",
    ").set_index('feat_name')\n",
    "\n",
    "df_feat_rf = pd.DataFrame(\n",
    "    {\n",
    "        'feat_name': feat_names,\n",
    "        'random_forest': mdl_rf.feature_importances_\n",
    "    }\n",
    ").set_index('feat_name')\n",
    "\n",
    "\n",
    "df_feat_dt = pd.DataFrame(\n",
    "    {\n",
    "        'feat_name': feat_names,\n",
    "        'decision_tree': mdl_dt.feature_importances_\n",
    "    }\n",
    ").set_index('feat_name')\n",
    "\n",
    "\n",
    "df_feat_merge = pd.concat(\n",
    "    [df_feat_cat.rename(columns = {'LossFunctionChange' : 'catboost'})['catboost'],   # catboost feature importance\n",
    "     df_feat_xgb['xgboost'],             # xgboost feature importance\n",
    "     df_feat_lgb['lightgbm'],              # lightgbm feature importance\n",
    "     df_feat_rf['random_forest'],              # random forest feature importance\n",
    "     df_feat_logit['logistic'],         # logit rigid cofficient\n",
    "     df_feat_dt['decision_tree']               # Decision tree feature importance\n",
    "     ], axis =1,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame (\n",
    "    data = df_feat_merge\n",
    "    index = df_feat_merge.index,\n",
    "    columns = df_feat_merge.columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    df_feat_merge\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_logit_set.difference(feat_xgb_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_logit_set.intersection(feat_cat_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(\n",
    "    {\n",
    "        'feat_name' : X.columns,\n",
    "        'feat_importance': mdl_rf.feature_importances_\n",
    "    }\n",
    ")\n",
    "\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mdl_xgb)\n",
    "shap_values = explainer(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary_legacy() got an unexpected keyword argument 'max_leaves'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [254], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary_plot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeat_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_leaves\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: summary_legacy() got an unexpected keyword argument 'max_leaves'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "shap.summary_plot(\n",
    "    shap_values = shap_values,\n",
    "    features = X_test,\n",
    "    feature_names = feat_names,\n",
    "    max_leaves = 15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "res = permutation_importance(mdl_rf, X_test, y_test, n_repeats = 10, random_state = 43, n_jobs =2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.describe(mdl_xgb.predict_proba(X_test)[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(mdl_lgb, X_test,['DAYS_BIRTH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(mdl_lgb, X_test,['AMT_ANNUITY', 'EXT_SOURCE_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(mdl_lgb, X_test,['NEW_EXT_SOURCES_MEAN', 'NEW_CREDIT_TO_ANNUITY_RATIO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X_test.iloc[:1000,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_mdi = mdl_rf.feature_importances_\n",
    "std = np.std([d_tree.feature_importances_ for d_tree in mdl_rf.estimators_])\n",
    "ser_feat_imp_mdi = pd.Series(feat_imp_mdi, index = feat_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "\n",
    "ser_feat_imp_mdi.sort_values(ascending= False).head(50).plot.barh(yerr = std)\n",
    "\n",
    "ax.set_title('Feature importances using MDI')\n",
    "ax.set_ylabel('Mean decrease in impurity')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(mdl_lgb)\n",
    "shap_values = explainer.shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_test.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][:100,:], X_test.iloc[:100,:])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in df_feat_lgb.head(10).index:\n",
    "    shap.dependence_plot(name, shap_values[1], X_test, display_features=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
